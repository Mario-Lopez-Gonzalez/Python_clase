{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d287d88e",
   "metadata": {
    "id": "d287d88e"
   },
   "source": [
    "# LISTA 7 - OBTENCIÃ“N DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f29b17",
   "metadata": {},
   "source": [
    "## <ins>Ejercicios Obligatorios </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99190d1",
   "metadata": {
    "id": "f99190d1"
   },
   "source": [
    "## EJERCICIO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "432c27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13305/822011591.py:245: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query(\"SELECT * FROM products LIMIT 3\", dataset)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 245\u001b[0m\n\u001b[1;32m    243\u001b[0m dataset\u001b[38;5;241m.\u001b[39mupdate(build_network_dataset(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeople\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    244\u001b[0m dataset\u001b[38;5;241m.\u001b[39mupdate(build_shopping_dataset(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeople\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddresses\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m--> 245\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_sql_query(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM products LIMIT 3\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[1;32m    527\u001b[0m         sql,\n\u001b[1;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[1;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2729\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[1;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/sql.py:2672\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery must be a string unless using sqlalchemy.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2671\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[0;32m-> 2672\u001b[0m cur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'cursor'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "import pymysql\n",
    "import random\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "NUMERO_CLIENTES = 500\n",
    "NUMERO_PROVEEDORES = 10\n",
    "SEMILLA_ALEATORIA_GENERADOR = 10\n",
    "SEMILLA_ALEATORIA_RANDOM = 1\n",
    "\n",
    "Faker.seed(SEMILLA_ALEATORIA_GENERADOR)\n",
    "random.seed(SEMILLA_ALEATORIA_RANDOM)\n",
    "fake = Faker(['es_ES'])\n",
    "\n",
    "def build_providers_dataset(number):\n",
    "  providers = []\n",
    "  for i in range(1, number+1):\n",
    "    providers.append({\n",
    "      \"provider_id\": i,\n",
    "      \"name\": fake.company(),\n",
    "      \"email\": fake.company_email(),\n",
    "      \"webpage\": fake.domain_name()\n",
    "    })\n",
    "\n",
    "  return {\n",
    "      \"providers\": providers\n",
    "  }\n",
    "\n",
    "def build_products_dataset(providers_info):\n",
    "  products = []\n",
    "  url = 'https://drive.google.com/uc?export=view&id=1D9MY0au4b7SXwhUdm6TNfsKfYzdkbAh_'\n",
    "  content = requests.get(url)\n",
    "  text = codecs.iterdecode(content.iter_lines(), 'utf-8')\n",
    "  reader = csv.DictReader(text, delimiter=',', quotechar='\"')\n",
    "  for row in reader:\n",
    "    products.append(row)\n",
    "\n",
    "  categories = sorted(set([product['category'] for product in products]))\n",
    "  categories = [{\"category_id\": i+1, \"name\": category} for (i, category) in enumerate(categories)]\n",
    "  categories_by_name = {category[\"name\"]: category[\"category_id\"] for category in categories}\n",
    "  products = [{\"product_id\": i+1, \n",
    "              \"name\": product[\"name\"], \n",
    "              \"price\": float(product[\"price\"]), \n",
    "              \"category_id\": categories_by_name[product[\"category\"]],\n",
    "              \"provider_id\": random.choice(providers_info)[\"provider_id\"]} \n",
    "              for (i, product) in enumerate(products)]\n",
    "  return {\n",
    "      'products': products,\n",
    "      'categories': categories\n",
    "  }\n",
    "\n",
    "def build_people_dataset(number):\n",
    "\n",
    "  people = []\n",
    "  addresses = []\n",
    "  payment_info = []\n",
    "  address_id = 0\n",
    "  payment_id = 0\n",
    "\n",
    "  for i in range(1, number+1):\n",
    "    # Person data\n",
    "    people.append({\n",
    "      \"person_id\": i,\n",
    "      \"first_name\": fake.first_name(),\n",
    "      \"last_name\": fake.last_name(),\n",
    "      \"birth_date\": fake.date_between_dates(datetime(1960, 1, 1), datetime(2002, 6, 1)),\n",
    "      \"email\": fake.email(),\n",
    "      \"phone\": fake.phone_number(),\n",
    "      \"username\": fake.user_name(),\n",
    "      \"password\": fake.sha256(),\n",
    "      \"job\": fake.job()\n",
    "    })\n",
    "\n",
    "    # Payment information\n",
    "    if random.choice([False]*1 + [True]*2):\n",
    "      payment_id += 1\n",
    "      payment_info.append({\n",
    "          \"payment_id\": payment_id,\n",
    "          \"person_id\": i,\n",
    "          \"expiration\": fake.credit_card_expire(),\n",
    "          \"number\": fake.credit_card_number(),\n",
    "          \"provider\": fake.credit_card_provider(),\n",
    "          \"security_code\": fake.credit_card_security_code()\n",
    "      })\n",
    "\n",
    "    # Registered addresses\n",
    "    for j in range(random.choice([1]*43 + [2]*6 + [3])):\n",
    "      address_id+=1\n",
    "      addresses.append(\n",
    "      {\n",
    "        \"address_id\": address_id,\n",
    "        \"person_id\": i,\n",
    "        \"city\": fake.city(),\n",
    "        \"number\": fake.building_number(),\n",
    "        \"country\": \"EspaÃ±a\",\n",
    "        \"zipcode\": fake.postcode(),\n",
    "        \"street\": fake.street_name()\n",
    "      })\n",
    "\n",
    "  return {\n",
    "      \"people\": people,\n",
    "      \"addresses\": addresses,\n",
    "      \"payment_information\": payment_info,\n",
    "  }\n",
    "\n",
    "def build_network_dataset(people_info):\n",
    "\n",
    "  WEB_PAGES = [fake.uri_path() for i in range(0,100)]\n",
    "  ACCESS_METHOD_PROPORTION = ['GET'] * 10 + ['POST'] \n",
    "  pages = []\n",
    "  accesses = []\n",
    "  access_id = 0\n",
    "\n",
    "  for i in range(0, len(WEB_PAGES)):\n",
    "    pages.append({\n",
    "        \"page_id\": i+1,\n",
    "        \"path\": WEB_PAGES[i]\n",
    "    })\n",
    "\n",
    "  for person in people_info:\n",
    "    # Access to webpages\n",
    "    for j in range(int(random.gauss(60, 40))):\n",
    "      access_id += 1\n",
    "      accesses.append({\n",
    "          \"access_id\": access_id,\n",
    "          \"person_id\": person[\"person_id\"],\n",
    "          \"method\": random.choice(ACCESS_METHOD_PROPORTION),\n",
    "          \"ip\": fake.ipv4_public(),\n",
    "          \"date\": fake.date_time_between(datetime(2020,1,1,0,0,0), datetime(2020,9,1,23,59,59)),\n",
    "          \"page_id\": random.randint(1, len(WEB_PAGES)-1)\n",
    "      })\n",
    "\n",
    "  # Anonymous access\n",
    "  for i in range(int(random.gauss(1000, 100))):\n",
    "    access_id += 1\n",
    "    accesses.append({\n",
    "        \"access_id\": access_id,\n",
    "        \"person_id\": None,\n",
    "        \"method\": random.choice(ACCESS_METHOD_PROPORTION),\n",
    "        \"ip\": fake.ipv4_public(),\n",
    "        \"date\": fake.date_time_between(datetime(2020,1,1,0,0,0), datetime(2020,9,1,23,59,59)),\n",
    "        \"page_id\": random.randint(1, len(WEB_PAGES)-1)\n",
    "    })\n",
    "\n",
    "  return {\n",
    "    \"web_pages\":  pages,\n",
    "    \"accesses\": accesses\n",
    "  }\n",
    "\n",
    "def build_shopping_dataset(people, products, people_addresses):\n",
    "\n",
    "  shopping_carts = []\n",
    "  shopping_cart_products = []\n",
    "  orders = []\n",
    "  order_products = []\n",
    "  invoices = []\n",
    "  cart_id = 0\n",
    "  shopping_cart_id = 0\n",
    "  order_id = 0\n",
    "  order_product_id = 0\n",
    "  invoice_id = 0\n",
    "\n",
    "  PRODUCTS_PROBABILITY = [1]*2 + [2] * 3 + [3] * 3 + [4]*2 + [5]\n",
    "  ORDER_PROBABILITY = [0]+[1]*7+[2]*3+[3]*3+[4]*2+[5]\n",
    "  QUANTITY_PROBABILITY = [1]*5 +[2]*2 +[3]\n",
    "  RATING_PROBABILITY = [1]+[2]+[3]*2+[4]*4+[5]*3\n",
    "\n",
    "  for person in people:\n",
    "    # Build shopping cart\n",
    "    if random.choice([False * 9] + [True]):\n",
    "      cart_id += 1\n",
    "      shopping_carts.append({\n",
    "          \"cart_id\": cart_id,\n",
    "          \"person_id\": person[\"person_id\"],\n",
    "          \"date\": fake.date_time_between(datetime(2020,1,1,0,0,0), datetime(2020,9,1,23,59,59)),\n",
    "      })\n",
    "\n",
    "      chosen = random.sample(products, k = random.choice(PRODUCTS_PROBABILITY))\n",
    "      for product in chosen:\n",
    "        shopping_cart_id += 1\n",
    "        shopping_cart_products.append({\n",
    "            \"cart_id\": cart_id,\n",
    "            \"product_id\": product[\"product_id\"],\n",
    "            \"quantity\": random.choice(QUANTITY_PROBABILITY)\n",
    "        })\n",
    "    \n",
    "    # Build orders\n",
    "    for i in range(0, random.choice(ORDER_PROBABILITY)):\n",
    "      order_id += 1\n",
    "      order_price = 0\n",
    "      chosen = random.sample(products, k = random.choice(PRODUCTS_PROBABILITY))\n",
    "      for product in chosen:\n",
    "        order_product_id += 1\n",
    "        quantity = random.choice(QUANTITY_PROBABILITY)\n",
    "        order_products.append({\n",
    "            \"order_id\": order_id,\n",
    "            \"product_id\": product[\"product_id\"],\n",
    "            \"quantity\": quantity\n",
    "        })\n",
    "        order_price += quantity * product['price']\n",
    "\n",
    "      person_addresses = [address for address in people_addresses if address[\"person_id\"] == person[\"person_id\"]]\n",
    "      delivery_address = random.choice(person_addresses)\n",
    "      billing_address = random.choice(person_addresses)\n",
    "      orders.append({\n",
    "          \"order_id\": order_id,\n",
    "          \"person_id\": person[\"person_id\"],\n",
    "          \"date\": fake.date_time_between(datetime(2020,1,1,0,0,0), datetime(2020,9,1,23,59,59)),\n",
    "          # Purposely left wrong\n",
    "          \"delivery_address\": delivery_address['address_id'],\n",
    "          \"billing_address\": billing_address['address_id'],\n",
    "          \"price\": order_price\n",
    "      })\n",
    "\n",
    "  # Build invoices\n",
    "  for order in random.choices(orders, k = int(len(orders) * 0.8)):\n",
    "    invoice_id += 1\n",
    "    invoices.append({\n",
    "      \"invoice_id\": invoice_id,\n",
    "      \"order_id\": order[\"order_id\"],\n",
    "      \"date\": fake.date_time_between(order[\"date\"], datetime(2020,9,1,23,59,59)),\n",
    "      \"rating\": random.choice(RATING_PROBABILITY)\n",
    "    })\n",
    "\n",
    "  return {\n",
    "      'carts': shopping_carts,\n",
    "      'cart_product': shopping_cart_products,\n",
    "      'orders': orders,\n",
    "      'order_product': order_products,\n",
    "      'invoices': invoices    \n",
    "  }\n",
    "\n",
    "dataset = {}\n",
    "dataset.update(build_providers_dataset(NUMERO_PROVEEDORES))\n",
    "dataset.update(build_products_dataset(dataset['providers']))\n",
    "dataset.update(build_people_dataset(NUMERO_CLIENTES))\n",
    "dataset.update(build_network_dataset(dataset['people']))\n",
    "dataset.update(build_shopping_dataset(dataset['people'], dataset['products'], dataset['addresses']))\n",
    "pd.read_sql_query(\"SELECT * FROM products LIMIT 3\", dataset) # A ratos falla el random y sino es que dict no tiene cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3836703",
   "metadata": {
    "id": "b3836703"
   },
   "source": [
    "La base de datos shop tiene una tabla people con informaciÃ³n sobre los clientes de la tienda ficticia. Escribe el cÃ³digo Python necesario para inicializar un DataFrame con el contenido de la tabla:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68e82b",
   "metadata": {
    "id": "4d68e82b"
   },
   "source": [
    "## EJERCICIO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a792237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FECHA</th>\n",
       "      <th>EST</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>PARAJE</th>\n",
       "      <th>TMED</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>HRMED</th>\n",
       "      <th>HRMAX</th>\n",
       "      <th>HRMIN</th>\n",
       "      <th>PREC</th>\n",
       "      <th>RADMED</th>\n",
       "      <th>VVMED</th>\n",
       "      <th>VVMAX</th>\n",
       "      <th>DVMED</th>\n",
       "      <th>ETO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/18</td>\n",
       "      <td>AL31</td>\n",
       "      <td>Totana</td>\n",
       "      <td>Lebor</td>\n",
       "      <td>12.87</td>\n",
       "      <td>19.17</td>\n",
       "      <td>5.909</td>\n",
       "      <td>46.44</td>\n",
       "      <td>75.90</td>\n",
       "      <td>22.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.03</td>\n",
       "      <td>2.05</td>\n",
       "      <td>6.821</td>\n",
       "      <td>249.66</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/01/18</td>\n",
       "      <td>AL31</td>\n",
       "      <td>Totana</td>\n",
       "      <td>Lebor</td>\n",
       "      <td>13.05</td>\n",
       "      <td>20.99</td>\n",
       "      <td>8.200</td>\n",
       "      <td>44.91</td>\n",
       "      <td>64.69</td>\n",
       "      <td>23.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.900</td>\n",
       "      <td>272.61</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/01/18</td>\n",
       "      <td>AL31</td>\n",
       "      <td>Totana</td>\n",
       "      <td>Lebor</td>\n",
       "      <td>13.57</td>\n",
       "      <td>22.57</td>\n",
       "      <td>7.060</td>\n",
       "      <td>61.66</td>\n",
       "      <td>81.00</td>\n",
       "      <td>37.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.63</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.077</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/01/18</td>\n",
       "      <td>AL31</td>\n",
       "      <td>Totana</td>\n",
       "      <td>Lebor</td>\n",
       "      <td>14.52</td>\n",
       "      <td>23.84</td>\n",
       "      <td>8.140</td>\n",
       "      <td>63.91</td>\n",
       "      <td>88.60</td>\n",
       "      <td>33.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.25</td>\n",
       "      <td>1.12</td>\n",
       "      <td>4.753</td>\n",
       "      <td>291.87</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/01/18</td>\n",
       "      <td>AL31</td>\n",
       "      <td>Totana</td>\n",
       "      <td>Lebor</td>\n",
       "      <td>10.36</td>\n",
       "      <td>17.70</td>\n",
       "      <td>4.565</td>\n",
       "      <td>77.86</td>\n",
       "      <td>99.90</td>\n",
       "      <td>44.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.46</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3.665</td>\n",
       "      <td>260.43</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17742</th>\n",
       "      <td>27/12/18</td>\n",
       "      <td>TP91</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>10.76</td>\n",
       "      <td>16.03</td>\n",
       "      <td>7.390</td>\n",
       "      <td>88.28</td>\n",
       "      <td>99.35</td>\n",
       "      <td>65.07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>93.96</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.626</td>\n",
       "      <td>266.05</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17743</th>\n",
       "      <td>28/12/18</td>\n",
       "      <td>TP91</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>11.19</td>\n",
       "      <td>15.81</td>\n",
       "      <td>6.600</td>\n",
       "      <td>83.62</td>\n",
       "      <td>96.95</td>\n",
       "      <td>58.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.32</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.724</td>\n",
       "      <td>320.25</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17744</th>\n",
       "      <td>29/12/18</td>\n",
       "      <td>TP91</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>12.23</td>\n",
       "      <td>14.68</td>\n",
       "      <td>10.190</td>\n",
       "      <td>91.24</td>\n",
       "      <td>98.00</td>\n",
       "      <td>73.23</td>\n",
       "      <td>9.2</td>\n",
       "      <td>35.73</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.978</td>\n",
       "      <td>15.21</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17745</th>\n",
       "      <td>30/12/18</td>\n",
       "      <td>TP91</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>11.01</td>\n",
       "      <td>15.29</td>\n",
       "      <td>6.700</td>\n",
       "      <td>85.61</td>\n",
       "      <td>96.72</td>\n",
       "      <td>62.92</td>\n",
       "      <td>0.2</td>\n",
       "      <td>108.46</td>\n",
       "      <td>1.13</td>\n",
       "      <td>4.802</td>\n",
       "      <td>15.22</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17746</th>\n",
       "      <td>31/12/18</td>\n",
       "      <td>TP91</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>Torre Pacheco</td>\n",
       "      <td>9.32</td>\n",
       "      <td>18.85</td>\n",
       "      <td>3.730</td>\n",
       "      <td>74.97</td>\n",
       "      <td>96.87</td>\n",
       "      <td>34.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.35</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.724</td>\n",
       "      <td>299.72</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17747 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FECHA   EST      MUNICIPIO         PARAJE   TMED   TMAX    TMIN  \\\n",
       "0      01/01/18  AL31         Totana          Lebor  12.87  19.17   5.909   \n",
       "1      02/01/18  AL31         Totana          Lebor  13.05  20.99   8.200   \n",
       "2      03/01/18  AL31         Totana          Lebor  13.57  22.57   7.060   \n",
       "3      04/01/18  AL31         Totana          Lebor  14.52  23.84   8.140   \n",
       "4      05/01/18  AL31         Totana          Lebor  10.36  17.70   4.565   \n",
       "...         ...   ...            ...            ...    ...    ...     ...   \n",
       "17742  27/12/18  TP91  Torre Pacheco  Torre Pacheco  10.76  16.03   7.390   \n",
       "17743  28/12/18  TP91  Torre Pacheco  Torre Pacheco  11.19  15.81   6.600   \n",
       "17744  29/12/18  TP91  Torre Pacheco  Torre Pacheco  12.23  14.68  10.190   \n",
       "17745  30/12/18  TP91  Torre Pacheco  Torre Pacheco  11.01  15.29   6.700   \n",
       "17746  31/12/18  TP91  Torre Pacheco  Torre Pacheco   9.32  18.85   3.730   \n",
       "\n",
       "       HRMED  HRMAX  HRMIN  PREC  RADMED  VVMED  VVMAX   DVMED   ETO  \n",
       "0      46.44  75.90  22.91   0.0  124.03   2.05  6.821  249.66  2.23  \n",
       "1      44.91  64.69  23.74   0.0  122.80   1.69  8.900  272.61  2.24  \n",
       "2      61.66  81.00  37.48   0.0  124.63   0.85  3.077   13.25  1.34  \n",
       "3      63.91  88.60  33.87   0.0  124.25   1.12  4.753  291.87  1.67  \n",
       "4      77.86  99.90  44.64   0.0  125.46   0.99  3.665  260.43  1.13  \n",
       "...      ...    ...    ...   ...     ...    ...    ...     ...   ...  \n",
       "17742  88.28  99.35  65.07   0.2   93.96   0.74  3.626  266.05  0.79  \n",
       "17743  83.62  96.95  58.85   0.0   93.32   0.66  3.724  320.25  0.80  \n",
       "17744  91.24  98.00  73.23   9.2   35.73   1.35  5.978   15.21  0.81  \n",
       "17745  85.61  96.72  62.92   0.2  108.46   1.13  4.802   15.22  0.92  \n",
       "17746  74.97  96.87  34.89   0.0  124.35   0.91  3.724  299.72  1.19  \n",
       "\n",
       "[17747 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "meteo_df = pd.read_csv(\"https://datosabiertos.carm.es/odata/Agricultura/IMIDA_dia_2018.csv\",\n",
    "                      header=0, sep=\";\", decimal=\",\", quotechar=\"\\\"\", encoding='ISO-8859-1',) # El archivo es UTF-8 pero no me lo reconoce bien, en internet se sugiere usar ISO\n",
    "meteo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f3b2a",
   "metadata": {
    "id": "ec7f3b2a"
   },
   "source": [
    "La siguiente URL https://datosabiertos.carm.es/odata/Agricultura/IMIDA_dia_2018.csv contiene el informe meteorolÃ³gico diario de las diferentes estaciones meteorolÃ³gicas de la RegiÃ³n de Murcia a lo largo del aÃ±o 2018. Observa el contenido del fichero csv y a continuaciÃ³n utiliza la funciÃ³n read_csv de pandas sobre esta URL con los  parÃ¡metros necesarios (header, sep, decimal, quotechar y encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c943554",
   "metadata": {
    "id": "5c943554"
   },
   "source": [
    "## EJERCICIO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbd95f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   provincia_iso  TOTAL_CASOS\n",
      "0              A        95133\n",
      "1             AB        22129\n",
      "2             AL        32810\n",
      "3             AV         9576\n",
      "4              B       346896\n",
      "5             BA        39881\n",
      "6             BI        64523\n",
      "7             BU        26136\n",
      "8              C        37876\n",
      "9             CA        47527\n",
      "10            CC        20478\n",
      "11            CE         3758\n",
      "12            CO        35542\n",
      "13            CR        35943\n",
      "14            CS        27077\n",
      "15            CU        15600\n",
      "16            GC        17267\n",
      "17            GI        47214\n",
      "18            GR        57198\n",
      "19            GU        15579\n",
      "20             H        18292\n",
      "21            HU        14862\n",
      "22             J        34356\n",
      "23             L        31092\n",
      "24            LE        27909\n",
      "25            LO        23899\n",
      "26            LU         9097\n",
      "27             M       493480\n",
      "28            MA        60913\n",
      "29            ME         5673\n",
      "30            MU        90861\n",
      "31            NC        20932\n",
      "32             O        33781\n",
      "33            OR        11751\n",
      "34             P        13363\n",
      "35            PM        49611\n",
      "36            PO        28027\n",
      "37             S        21694\n",
      "38            SA        23023\n",
      "39            SE        76642\n",
      "40            SG        11380\n",
      "41            SO         5648\n",
      "42            SS        44448\n",
      "43             T        42665\n",
      "44            TE        10360\n",
      "45            TF        15412\n",
      "46            TO        50299\n",
      "47             V       147592\n",
      "48            VA        40768\n",
      "49            VI        19204\n",
      "50             Z        67564\n",
      "51            ZA        10511\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "covid_df = pd.read_csv(\"https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_provincia.csv\",\n",
    "                      header=0, sep=\",\", decimal=\".\", quotechar=\"\\\"\", encoding='ISO-8859-1',) # El archivo es UTF-8 pero no me lo reconoce bien, en internet se sugiere usar ISO\n",
    "print(covid_df.groupby('provincia_iso')['num_casos'].sum().reset_index().rename(columns={'num_casos': 'TOTAL_CASOS'})) # Equivale a SELECT SUM(NUM_CASOS) AS 'TOTAL_CASOS' FROM CSV GROUP_BY PROVINCIA_ISO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe60bc",
   "metadata": {
    "id": "f2fe60bc"
   },
   "source": [
    "El Instituto de Salud Carlos III ofrece gratuitamente informaciÃ³n actualizada sobre la situaciÃ³n del COVID-19 en EspaÃ±a. En la siguiente URL podemos obtener un fichero csv actualizado con los casos positivos notificados por las Comunidades AutÃ³nomas a nivel provincial: https://cnecovid.isciii.es/covid19/resources/casos_diagnostico_provincia.csv. En el siguiente enlace se nos describe este conjunto de datos y se nos proporcionan otros conjuntos de datos de interÃ©s.\n",
    "\n",
    "Utiliza el mÃ©todo read_csv con los parÃ¡metros adecuados para obtener el DataFrame con los casos positivos por provincia. Muestra a continuaciÃ³n su contenido:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfb36c",
   "metadata": {
    "id": "cbbfb36c"
   },
   "source": [
    "## EJERCICIO 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435b7eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>cast</th>\n",
       "      <th>genres</th>\n",
       "      <th>href</th>\n",
       "      <th>extract</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>thumbnail_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After Dark in Central Park</td>\n",
       "      <td>1900</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boarding School Girls' Pajama Parade</td>\n",
       "      <td>1900</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buffalo Bill's Wild West Parad</td>\n",
       "      <td>1900</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caught</td>\n",
       "      <td>1900</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clowns Spinning Hats</td>\n",
       "      <td>1900</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Silent]</td>\n",
       "      <td>Clowns_Spinning_Hats</td>\n",
       "      <td>Clowns Spinning Hats is a black-and-white sile...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36268</th>\n",
       "      <td>Aquaman and the Lost Kingdom</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Jason Momoa, Amber Heard, Willem Dafoe, Patri...</td>\n",
       "      <td>[Superhero]</td>\n",
       "      <td>Aquaman_and_the_Lost_Kingdom</td>\n",
       "      <td>Aquaman and the Lost Kingdom is an upcoming Am...</td>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/en/thum...</td>\n",
       "      <td>320.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36269</th>\n",
       "      <td>Untitled Ghostbusters: Afterlife sequel</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Mckenna Grace, Carrie Coon, Finn Wolfhard, Pa...</td>\n",
       "      <td>[Comedy, Supernatural]</td>\n",
       "      <td>Untitled_Ghostbusters:_Afterlife_sequel</td>\n",
       "      <td>The untitled Ghostbusters: Afterlife sequel is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36270</th>\n",
       "      <td>Rebel Moon</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Sofia Boutella, Charlie Hunnam, Ray Fisher, D...</td>\n",
       "      <td>[Science Fiction]</td>\n",
       "      <td>Rebel_Moon</td>\n",
       "      <td>Rebel Moon is an upcoming American epic space ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36271</th>\n",
       "      <td>Migration</td>\n",
       "      <td>2023</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Migration_(2023_film)</td>\n",
       "      <td>This is a list of productions produced by Illu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36272</th>\n",
       "      <td>The Color Purple</td>\n",
       "      <td>2023</td>\n",
       "      <td>[Fantasia Barrino, Colman Domingo, Corey Hawki...</td>\n",
       "      <td>[Drama, Musical, Historical]</td>\n",
       "      <td>The_Color_Purple_(2023_film)</td>\n",
       "      <td>The Color Purple is an upcoming American music...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36273 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  year  \\\n",
       "0                   After Dark in Central Park  1900   \n",
       "1         Boarding School Girls' Pajama Parade  1900   \n",
       "2               Buffalo Bill's Wild West Parad  1900   \n",
       "3                                       Caught  1900   \n",
       "4                         Clowns Spinning Hats  1900   \n",
       "...                                        ...   ...   \n",
       "36268             Aquaman and the Lost Kingdom  2023   \n",
       "36269  Untitled Ghostbusters: Afterlife sequel  2023   \n",
       "36270                               Rebel Moon  2023   \n",
       "36271                                Migration  2023   \n",
       "36272                         The Color Purple  2023   \n",
       "\n",
       "                                                    cast  \\\n",
       "0                                                     []   \n",
       "1                                                     []   \n",
       "2                                                     []   \n",
       "3                                                     []   \n",
       "4                                                     []   \n",
       "...                                                  ...   \n",
       "36268  [Jason Momoa, Amber Heard, Willem Dafoe, Patri...   \n",
       "36269  [Mckenna Grace, Carrie Coon, Finn Wolfhard, Pa...   \n",
       "36270  [Sofia Boutella, Charlie Hunnam, Ray Fisher, D...   \n",
       "36271                                                 []   \n",
       "36272  [Fantasia Barrino, Colman Domingo, Corey Hawki...   \n",
       "\n",
       "                             genres                                     href  \\\n",
       "0                                []                                     None   \n",
       "1                                []                                     None   \n",
       "2                                []                                     None   \n",
       "3                                []                                     None   \n",
       "4                          [Silent]                     Clowns_Spinning_Hats   \n",
       "...                             ...                                      ...   \n",
       "36268                   [Superhero]             Aquaman_and_the_Lost_Kingdom   \n",
       "36269        [Comedy, Supernatural]  Untitled_Ghostbusters:_Afterlife_sequel   \n",
       "36270             [Science Fiction]                               Rebel_Moon   \n",
       "36271                            []                    Migration_(2023_film)   \n",
       "36272  [Drama, Musical, Historical]             The_Color_Purple_(2023_film)   \n",
       "\n",
       "                                                 extract  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4      Clowns Spinning Hats is a black-and-white sile...   \n",
       "...                                                  ...   \n",
       "36268  Aquaman and the Lost Kingdom is an upcoming Am...   \n",
       "36269  The untitled Ghostbusters: Afterlife sequel is...   \n",
       "36270  Rebel Moon is an upcoming American epic space ...   \n",
       "36271  This is a list of productions produced by Illu...   \n",
       "36272  The Color Purple is an upcoming American music...   \n",
       "\n",
       "                                               thumbnail  thumbnail_width  \\\n",
       "0                                                    NaN              NaN   \n",
       "1                                                    NaN              NaN   \n",
       "2                                                    NaN              NaN   \n",
       "3                                                    NaN              NaN   \n",
       "4                                                    NaN              NaN   \n",
       "...                                                  ...              ...   \n",
       "36268  https://upload.wikimedia.org/wikipedia/en/thum...            320.0   \n",
       "36269                                                NaN              NaN   \n",
       "36270                                                NaN              NaN   \n",
       "36271                                                NaN              NaN   \n",
       "36272                                                NaN              NaN   \n",
       "\n",
       "       thumbnail_height  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "...                 ...  \n",
       "36268             163.0  \n",
       "36269               NaN  \n",
       "36270               NaN  \n",
       "36271               NaN  \n",
       "36272               NaN  \n",
       "\n",
       "[36273 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = pd.read_json(\"https://raw.githubusercontent.com/prust/wikipedia-movie-data/master/movies.json\")\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0211a75",
   "metadata": {
    "id": "f0211a75"
   },
   "source": [
    "La siguiente URL contiene informaciÃ³n de pelÃ­culas estadounidenses obtenidas de la Wikipedia en formato JSON: https://raw.githubusercontent.com/prust/wikipedia-movie-data/master/movies.json. Utiliza el mÃ©todo `read_json` de `pandas` para cargar su contenido en un DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe1c72",
   "metadata": {
    "id": "88fe1c72"
   },
   "source": [
    "## EJERCICIO 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "871a99dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cala grifo de cocina negro mate caÃ±o giratorio Roca A5A846ENB0\n",
      "CALA Grifo diseÃ±o para cocina con caÃ±o giratorio Roca A5A846EC00\n",
      "CALA grifo monomando de cocina con caÃ±o giratorio en negro Roca A5A836ENB0\n",
      "CALA Mezclador para cocina con caÃ±o giratorio Roca A5A856EC00\n",
      "CALA Monomando de Cocina Roca A5A836EC00. Ofertas de grifos de cocina\n",
      "CARMEN Bimando cocina A5A844BC00 Roca\n",
      "Exclusivo CALA grifo para cocina Negro de Roca A5A856ENB0 calidad\n",
      "GLERA diseÃ±o ergonÃ³mico monomando cocina moderno Roca A5A834DC00\n",
      "GLERA Grifo fregadero extraÃ­ble diseÃ±o elegante para cocina A5A814DC00 Roca\n",
      "GLERA Mezclador monomando para cocina con caÃ±o giratorio Roca A5A844DC00\n",
      "GLERA Monomando para cocina con caÃ±o giratorio y ducha Roca A5A854DC00\n",
      "GLERA Pro - Mezclador monomando para cocina con caÃ±o giratorio con muelle Roca A5A8A4DC00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://catalogoreina.com/859-grifos-cocina-roca\"\n",
    "r = requests.get(URL) # Obtenemos el HTML en RAW\n",
    "\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser') # Limpiamos el HTML con el parser adecuado\n",
    "tags = html_soup.find_all('a', class_='product-name') # Buscamos todos los enlaces que tengan una clase con product-name\n",
    "for tag in tags: # Imprimimos solo el tÃ­tulo de los enlaces dado a que el texto mostrado por <a> es un recorte para que quede bien la pÃ¡gina\n",
    "    print(tag['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7446bb5",
   "metadata": {
    "id": "f7446bb5"
   },
   "source": [
    "Haciendo uso de la librerÃ­a `requests` y `BeautifulSoup`, accede a la siguiente URL https://catalogoreina.com/859-grifos-cocina-roca y recupera el nombre de los artÃ­culos mostrados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd805d",
   "metadata": {
    "id": "6bcd805d"
   },
   "source": [
    "## EJERCICIO 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73c07f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Oracle\n",
      "2 MySQL\n",
      "3 Microsoft\n",
      "4 PostgreSQL\n",
      "5 MongoDB\n",
      "6 Redis\n",
      "7 Snowflake\n",
      "8 Elasticsearch\n",
      "9 IBM\n",
      "10 SQLite\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://db-engines.com/en/ranking\"\n",
    "r = requests.get(URL)\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "table_tr = html_soup.select('table tr') # Saco toda la tabla a lo bruto\n",
    "top_10 = table_tr[6:16] # No me salÃ­a usando nth-child asi que he ido recortando a mano\n",
    "i = 0 # Contador para el numero de fila que quede bonito\n",
    "for row in top_10:\n",
    "    i += 1\n",
    "    print(i, end=\" \")\n",
    "    print(row.find('a').text.strip().split()[0]) # Saco el <a> del <tr>, le quito todo lo que no sea texto y de ahÃ­ solo quiero la primera palabra, tampoco se usar extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18202d6c",
   "metadata": {
    "id": "18202d6c"
   },
   "source": [
    "Haciendo uso de la librerÃ­a `requests` y `BeautifulSoup`, accede a la pÃ¡gina web del DB-Engines https://db-engines.com/en/ranking y recupera los nombres de las 10 bases de datos mÃ¡s populares:\n",
    "\n",
    "PISTA: el selector CSS [`nth-child`](https://developer.mozilla.org/es/docs/Web/CSS/:nth-child) puede serte de utilidad.\n",
    "\n",
    "OTRA PISTA: para evitar que salga el contenido de la etiqueta `info` junto al nombre de la base de datos, haz uso del mÃ©todo [`extract`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60749fd",
   "metadata": {
    "id": "e60749fd"
   },
   "source": [
    "## EJERCICIO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a4b8494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Muebles de baÃ±o ALTHEA de 3 cajones\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://catalogoreina.com/nuestras-marcas-muebles-bano/10233-mueble-bano-con-patas-althea-moderno-3-cajones.html\"\n",
    "r = requests.get(URL)\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "name = html_soup.find('h1').text.strip().replace(\".\",\"\") # Tiene puntos por uso estÃ©tico que se pueden quitar\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638efe3",
   "metadata": {},
   "source": [
    "Haciendo uso de la librerÃ­a `requests` y `BeautifulSoup`, recupera el nombre, los acabados, las medidas y el plazo de entrega del siguiente artÃ­culo: https://catalogoreina.com/nuestras-marcas-muebles-bano/10233-mueble-bano-con-patas-althea-moderno-3-cajones.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b822c",
   "metadata": {},
   "source": [
    "## <ins>Ejercicios Opcionales </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c95369",
   "metadata": {},
   "source": [
    "## EJERCICIO 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebf9a9",
   "metadata": {},
   "source": [
    "Esta base de datos tambiÃ©n contiene una tabla orders con informaciÃ³n de la cabecera de los pedidos de la tienda. En esta tabla existe una columna price que almacena el importe total del pedido. Construye a continuaciÃ³n un DataFrame con las cabeceras de pedidos ordenada por importe de manera descendente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dcec8",
   "metadata": {},
   "source": [
    "## EJERCICIO 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b544b",
   "metadata": {},
   "source": [
    "La base de datos tambiÃ©n contiene las tablas web_pages y accesses con informaciÃ³n de las pÃ¡ginas web de la empresa y de los accesos realizados a ellas respectivamente. Ambas tablas pueden relacionarse por el campo page_id. Inicializa un DataFrame que contenga los campos page_id y path de web_pages y el total de accesos realizado sobre cada una de ellas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2a0e3",
   "metadata": {},
   "source": [
    "## EJERCICIO 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178aee78",
   "metadata": {},
   "source": [
    "Crea un diccionario Python que represente una factura en formato JSON:\n",
    "\n",
    "* ContendrÃ¡ 4 campos cuyos valores puedes inventarte, pero siguiendo las siguientes indicaciones:\n",
    "  * **client_id**: identificador de cliente, de tipo cadena.\n",
    "  * **products**: array con al menos dos productos de tipo objeto. Cada objeto tendrÃ¡ dos campos:\n",
    "    * **name**: de tipo cadena.\n",
    "    * **price**: de tipo numÃ©rico.\n",
    "  * **date**: de tipo cadena.\n",
    "  * **address**: de tipo objeto, con los siguientes campos:\n",
    "    * **street**: de tipo objeto, con los siguientes campos:\n",
    "      * **name**: de tipo cadena.\n",
    "      * **number**: de tipo numÃ©rico.\n",
    "    * **zipcode**: de tipo numÃ©rico.\n",
    "\n",
    "El diccionario tendrÃ¡ que tener un formato tal que sea aceptado por el siguiente [validador](https://jsonlint.com/) de contenido JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618e719",
   "metadata": {},
   "source": [
    "## EJERCICIO 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b89ce5",
   "metadata": {},
   "source": [
    "Utilizando la librerÃ­a PyPDF4, recupera el nÃºmero de pÃ¡ginas del siguiente PDF:\n",
    "```\n",
    "PDF_URL = 'https://www.mscbs.gob.es/profesionales/saludPublica/ccayes/alertasActual/nCov/documentos/Actualizacion_278_COVID-19.pdf'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863214e7",
   "metadata": {},
   "source": [
    "## EJERCICIO 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065f31d",
   "metadata": {},
   "source": [
    "Utilizando la librerÃ­a PyPDF4, recupera la fecha de creaciÃ³n del documento (campo /CreationDate de los metadatos):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a27f6c",
   "metadata": {},
   "source": [
    "## EJERCICIO 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80dc86",
   "metadata": {},
   "source": [
    "Extrae la informaciÃ³n del pdf en tablas y obten aquella que contiene los Detalles de los quince paÃ­ses con mÃ¡s casos confirmados fuera de Europa.\n",
    "\n",
    "PISTA: esta tabla es la Ãºltima del documento PDF. Utiliza la funciÃ³n len para obtener el total de tablas extraÃ­das y saber cuÃ¡l de ellas seleccionar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31aded0",
   "metadata": {},
   "source": [
    "## EJERCICIO 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd167507",
   "metadata": {},
   "source": [
    "Mediante `read_excel`, carga el contenido de todas las hojas del fichero excel \"Orders-With Nulls.xlsx\", muestra el nombre de todas las hojas y muestra la hoja denominada `Summary`:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
